Introduction to machine learning concepts
Introduction


ML in many ways is an intersection between Data Science and Software Engineering. The goal of ML is to use data to create a predictive model that can be incorporated into software applications or service
The fundamental idea of ML is to use the past data to predict the unknown outcomes or values


Fundamentally a ML model is a software application that encapsulates a function to calculate an output value based on one or more input values. The process of defining that function is known as training. After the function has been defined, you can use it to predict new values in a process called inferencing.


The training data consists of past observations. In most cases, the observations include the observed attributes or features of the thing being observed, and the known value of the thing you want to train a model to predict (known as the label).
Label → what we predict, also called as target, output, y
Features → what we use to predict it, also called as predictors, independent variables or x


Example →
In the medical scenario, the goal is to predict whether or not a patient is at risk of diabetes based on their clinical measurements. The patient's measurements (weight, blood glucose level, and so on) are the features (x), and the likelihood of diabetes (for example, 1 for at risk, 0 for not at risk) is the label (y).


An algorithm is applied to the data to determine the relationship between the features and labels and generalize that relationship as a calculation that can be performed on x to calculate y
The result of the algorithm is a model that encapsulates the calculation derived by the algorithm as a function - let's call it f. In mathematical notation:
y = f(x)


Now that the training phase is complete, the trained model can be used for inferencing. The model is essentially a software program that encapsulates the function produced by the training process. You can input a set of feature values, and receive as an output a prediction of the corresponding label. Because the output from the model is a prediction that was calculated by the function, and not an observed value, you'll often see the output from the function shown as ŷ (which is rather delightfully verbalized as "y-hat").


Types of ML Models
  



Supervised ML → Where we have both features and labels in the training data. Supervised machine learning is used to train models by determining a relationship between the features and labels in past observations, so that unknown labels can be predicted for features in future cases.


Regression is a form of supervised ML in which the label that we predict is a numeric value, for instance, the number of ice creams sold on a given day, based on the temperature, rainfall, and windspeed.


Classification is a form of supervised ML in which the label represents a categorization or class.
Two types Binary and Multiclass
Binary Classification → Binary classification models predict one of two mutually exclusive outcomes
For example → Whether a patient is at risk for diabetes based on clinical metrics like weight, age, blood glucose level, and so on
The model predicts a binary true/false or positive/negative prediction for a single possible class.


Multiclass classification extends binary classification to predict a label that represents one of multiple possible classes. For example,
The species of a penguin (Adelie, Gentoo, or Chinstrap) based on its physical measurements.
The genre of a movie (comedy, horror, romance, adventure, or science fiction) based on its cast, director, and budget.
In most scenarios that involve a known set of multiple classes, multiclass classification is used to predict mutually exclusive labels. For example, a penguin can't be both a Gentoo and an Adelie. However, there are also some algorithms that you can use to train multilabel classification models, in which there may be more than one valid label for a single observation. For example, a movie could potentially be categorized as both science fiction and comedy.


Unsupervised machine learning involves training models using data that consists only of feature values without any known labels. Unsupervised machine learning algorithms determine relationships between the features of the observations in the training data.
The most common form of unsupervised machine learning is clustering. A clustering algorithm identifies similarities between observations based on their features, and groups them into discrete clusters. 
For example:
Group similar flowers based on their size, number of leaves, and number of petals.
Identify groups of similar customers based on demographic attributes and purchasing behavior.


Regression
  

Regression models are trained to predict numeric label values based on training data that includes both features and known labels. The process for training a regression model (or indeed, any supervised machine learning model) involves multiple iterations in which you use an appropriate algorithm (usually with some parameterized settings) to train a model, evaluate the model's predictive performance, and refine the model by repeating the training process with different algorithms and parameters until you achieve an acceptable level of predictive accuracy.


The diagram shows four key elements of the training process for supervised machine learning models:


Split the training data (randomly) to create a dataset with which to train the model while holding back a subset of the data that you'll use to validate the trained model.
Use an algorithm to fit the training data to a model. In the case of a regression model, use a regression algorithm such as linear regression.
Use the validation data you held back to test the model by predicting labels for the features.
Compare the known actual labels in the validation dataset to the labels that the model predicted. Then aggregate the differences between the predicted and actual label values to calculate a metric that indicates how accurately the model predicted for the validation data.
After each train, validate, and evaluate iteration, you can repeat the process with different algorithms and parameters until an acceptable evaluation metric is achieved.


In ML training, parameters = internal values the model learns from data.
They are not set by you — they are learned during training.
Linear Regression
Parameters = weights (𝑤) and bias (𝑏)
The model learns:
y=w1​x1​+w2​x2​+...+b
Parameters = the learned internal knobs that minimize error.
Hyperparameters = the settings YOU adjust (learning rate, number of trees, regularization)


Simple Analogy
Imagine you're teaching someone math.


Hyperparameters = Your teaching strategy
How fast you teach (learning rate)
How many days you teach (epochs)
Class size (batch size)


Parameters = What the student actually learns
Their knowledge (weights)
Their understanding (bias)


Example - Regression
We want to predict the ice-cream sales based on the temp of the day. Lets say we have some historical data which shows the sales and the temp of the day. We split the data into test and train and then use an algorithm like linear regression to find out the relationship between the feature(x) and the label(y).
Once we have the relationship or basically a function we can then check with our test data how accurate are the results and how far the actuals are vs the predicted values.
We also have certain metrics to check the actuals(y) vs the predicted(y hat) and below are few common metrics


Mean Absolute Error → This metric gives an average of the absolute variance from the actuals label. It doesn't matter if the variance is positive or negative it takes the absolute error for each prediction and then takes the average


Mean Squared Error → This metric helps to amplify the larger errors by squaring them and sometimes metrics like this shows how well the errors are smoothened. If the MSE is low it doesnt not mean the curve is smooth it just states that the predictions are closer to average.
MSE doesnt tell the frequency of errors but it tells the average size of errors, if the error is high the MSE will have a good spike due to squaring of the error.
MSE measures magnitude of error (with emphasis on large errors).
It DOES NOT tell you “smoothness,” “frequency,” or “distribution” of errors.


Root Mean Squared Error → MSE measures average squared error, which emphasizes large errors, but the squaring changes the scale. MSE is in squared units, so to interpret error in the same units as the label, we take the square root → RMSE.


Coefficient of Determination → R^2 → It measures the Proportion of variance in the actual target values that the model can explain.
It compares the sum of squared differences between predicted and actual labels with the sum of squared differences between the actual label values and the mean of actual label values, like this:
R2 = 1- ∑(y-ŷ)2 ÷ ∑(y-ȳ)2
Numerator = model error (SSE: Sum of Squared Errors)
Denominator = baseline error (SST: Total Sum of Squares)
If your model is better than the mean → R² goes closer to 1
If worse → R² becomes negative


The metrics described above are commonly used to evaluate a regression model. In most real-world scenarios, a data scientist will use an iterative process to repeatedly train and evaluate a model, varying:


Feature selection and preparation (choosing which features to include in the model, and calculations applied to them to help ensure a better fit).
Algorithm selection (We explored linear regression in the previous example, but there are many other regression algorithms)
Algorithm parameters (numeric settings to control algorithm behavior, more accurately called hyperparameters to differentiate them from the x and y parameters).
After multiple iterations, the model that results in the best evaluation metric that's acceptable for the specific scenario is selected.


Classification
Classification like regression is a supervised learning method and so it follows the same iterative path of training, validating and evaluating models.
Instead of calculating numeric values like a regression model, the algorithms used to train classification models calculate probability values for class assignment and the evaluation metrics used to assess model performance compare the predicted classes to the actual classes.
Binary Classification algorithms are used to train a model that predicts one of the two possible labels for a single class, essentially predicting True or False


Lets take an example where we have a dataset of blood sugar level and diabetics yes/no
We have a feature x called Blood Sugar Level and a label y which says 1 or 0 where 1 being diabetics Yes and 0 being diabetics No
To train the model, we'll use an algorithm to fit the training data to a function that calculates the probability of the class label being true (in other words, that the patient has diabetes). Probability is measured as a value between 0.0 and 1.0, such that the total probability for all possible classes is 1.0. So for example, if the probability of a patient having diabetes is 0.7, then there's a corresponding probability of 0.3 that the patient isn't diabetic.


There are many algorithms that can be used for binary classification, such as logistic regression, which derives a sigmoid (S-shaped) function with values between 0.0 and 1.0, like this:
  

Despite its name, in machine learning logistic regression is used for classification, not regression. The important point is the logistic nature of the function it produces, which describes an S-shaped curve between a lower and upper value (0.0 and 1.0 when used for binary classification).


We can see that for three observations the value of y is true and so the probability is 1 and similarly for rest three observations the value of y is false and so the probability is 0
The S-shaped curve describes the probability distribution so that plotting a value of x on the line identifies the corresponding probability that y is 1. Here we are concerned only about the fact that we want the probability of y as 1, y can be 0 or 1 but P(y) we want is 1 and the reverse is implicit where if we want P(y) = 0 then we do P(y) at 0 = 1 - P(y) at 1
The function produced by the algorithm describes the probability of y being true (y=1) for a given value of x. Mathematically, you can express the function like this:


f(x) = P(y=1 | x)
f of x equals the probability that y equals 1, given x.
What each part means (important)
f(x)
“The model’s output when input is x”
In logistic regression, this output is a probability, not a class


P( )
Stands for probability


y = 1
The event we care about
“The target belongs to class 1”


| x
Read as “given x”
Means the probability is conditional on x


So:
P(y=1∣x)
means:
“Among all cases with input x, what is the probability that the label is 1?”


The diagram also includes a horizontal line to indicate the threshold at which a model based on this function will predict true (1) or false (0). The threshold lies at the mid-point for y (P(y) = 0.5). For any values at this point or above, the model will predict true (1); while for any values below this point it will predict false (0).
Since the model only gives out the prediction and not the actual decision we have to define a threshold above which we can ask to predict it as 1 and below which as 0. This threshold can vary based on use cases


Binary classification evaluation metrics
The first step in calculating evaluation metrics for a binary classification model is usually to create a matrix of the number of correct and incorrect predictions for each possible class label:
  

This visualization is called a confusion matrix, and it shows the prediction totals where:


ŷ=0 and y=0: True negatives (TN)
ŷ=1 and y=0: False positives (FP)
ŷ=0 and y=1: False negatives (FN)
ŷ=1 and y=1: True positives (TP)


The True negative and True positive are on the diagonal of this matrix which can give a quick glance on the models accuracy based on the color intensity of the diagonal


Accuracy → Correct Predictions upon all predictions
The simplest metric you can calculate from the confusion matrix is accuracy - the proportion of predictions that the model got right


It can be given as 
(TN + TP) / (TN + FP + FN + TP)


Accuracy might initially seem like a good metric to evaluate a model, but consider this. Suppose 11% of the population has diabetes. You could create a model that always predicts 0 (a dumb model), and it would achieve an accuracy of 89%, even though it makes no real attempt to differentiate between patients by evaluating their features. What we really need is a deeper understanding of how the model performs at predicting 1 for positive cases and 0 for negative cases.
Basically accuracy fails when there is class imbalance in the dataset and may give high result despite it being not differentiate the classes itself.
How lets dig deeper
Lets say we have 100 people. 11 of them have diabetes and 89 of them do not have it. If we have a dumb model that predicts 0 for all the patients then the matrix will be like
TP → 0 (y = 1 and y^ = 1)
TN → 89 (y = 0 and y^ = 0)
FP → 0 (y = 0 and y^ = 1)
FN → 11 (y = 1 and y^ = 0)
And so if you see the accuracy will be 89% which is completely dumb to say because all the model does is just predict as 0 for all the cases and still gets a n accuracy of 89%


Recall →
Recall is a metric that measures the proportion of positive cases that the model identified correctly. In other words, compared to the number of patients who have diabetes, how many did the model predict to have diabetes?


Formula is TP / (TP + FN)


Out of all people who actually have diabetes, how many did we detect?
Recall focuses on False Negatives.
High recall → few FN
Low recall → many sick people missed


Precision → Precision is a similar metric to recall, but measures the proportion of predicted positive cases where the true label is actually positive. In other words, what proportion of the patients predicted by the model to have diabetes actually have diabetes?


Formula → TP / (TP + FP)
Out of all people the model predicted as diabetic, how many actually are?
Precision focuses on False Positives.
High precision → few FP
Low precision → many false alarms


If you:
Lower the threshold → predict more positives
Recall ↑ (fewer FN)
Precision ↓ (more FP)


If you:
Raise the threshold → predict fewer positives
Precision ↑
Recall ↓


You usually can’t maximize both.
Spam filter example:
Recall: How many actual spam emails did we catch?
Precision: Of the emails we marked as spam, how many were really spam?


Medical test:
Recall: Did we catch all sick patients?
Precision: Are the diagnosed patients actually sick?


F1 score → F1-score is an overall metric that combines recall and precision. The formula for F1-score is:


2 x ((Precision x Recall) ÷ (Precision + Recall))


The F1 score is the harmonic mean of precision and recall:
Why does harmonic mean and not the normal average?
Because we want a score that:
Is high only if both precision and recall are high
Drops sharply if either one is low


So F1 doesn’t “balance” them — it forces agreement.
Area Under the Curve (AUC)
Another name for recall is the true positive rate (TPR), and there's an equivalent metric called the false positive rate (FPR) that is calculated as FP÷(FP+TN). We already know that the TPR for our model when using a threshold of 0.5 is 0.75, and we can use the formula for FPR to calculate a value of 0÷2 = 0.


Of course, if we were to change the threshold above which the model predicts true (1), it would affect the number of positive and negative predictions; and therefore change the TPR and FPR metrics. These metrics are often used to evaluate a model by plotting a received operator characteristic (ROC) curve that compares the TPR and FPR for every possible threshold value between 0.0 and 1.0:
  

The ROC curve for a perfect model would go straight up the TPR axis on the left and then across the FPR axis at the top. Since the plot area for the curve measures 1x1, the area under this perfect curve would be 1.0 (meaning that the model is correct 100% of the time). In contrast, a diagonal line from the bottom-left to the top-right represents the results that would be achieved by randomly guessing a binary label; producing an area under the curve of 0.5. In other words, given two possible class labels, you could reasonably expect to guess correctly 50% of the time.


In the case of our diabetes model, the curve above is produced, and the area under the curve (AUC) metric is 0.875. Since the AUC is higher than 0.5, we can conclude the model performs better at predicting whether or not a patient has diabetes than randomly guessing.


The diagonal ROC line represents a random classifier where TPR increases at the same rate as FPR, giving an AUC of 0.5, meaning the model is no better than chance at distinguishing positive from negative cases — not that it is 50% accurate.


Multiclass classification →
Similar to binary classification but this time we have multiple labels. Lets say we have to distinguish the type of penguin, 0 → Adelie, 1 → Gentoo, 2 → Chinstrap
Two algorithms are popular
One vs Rest (OvR) algorithm →
One-vs-Rest algorithms train a binary classification function for each class, each calculating the probability that the observation is an example of the target class. Each function calculates the probability of the observation being a specific class compared to any other class. For our penguin species classification model, the algorithm would essentially create three binary classification functions:
f0(x) = P(y=0 | x)
f1(x) = P(y=1 | x)
f2(x) = P(y=2 | x)
Each algorithm produces a sigmoid function that calculates a probability value between 0.0 and 1.0. A model trained using this kind of algorithm predicts the class for the function that produces the highest probability output.


Multinomial Algorithm →
As an alternative approach is to use a multinomial algorithm, which creates a single function that returns a multi-valued output. The output is a vector (an array of values) that contains the probability distribution for all possible classes - with a probability score for each class which when totaled add up to 1.0:


f(x) =[P(y=0|x), P(y=1|x), P(y=2|x)]


An example of this kind of function is a softmax function, which could produce an output like the following example:


[0.2, 0.3, 0.5]


The elements in the vector represent the probabilities for classes 0, 1, and 2 respectively; so in this case, the class with the highest probability is 2.


Regardless of which type of algorithm is used, the model uses the resulting function to determine the most probable class for a given set of features (x) and predicts the corresponding class label (y).
  



Clustering
Clustering is a form of unsupervised machine learning in which observations are grouped into clusters based on similarities in their data values, or features. This kind of machine learning is considered unsupervised because it doesn't make use of previously known label values to train a model. In a clustering model, the label is the cluster to which the observation is assigned, based only on its features.


For example we have a dataset of vehicles and records say mode of transport, no of wheels, length and so on, these become the features and there are no labels in the dataset, just the features. The goal is not to identify the label but to group them into clusters based on the features
We need the clustering but in many real world scenarios we do not know the labels and hence it is important to identify and make clusters based on their features
Clustering is used when you’re exploring, not predicting


Customer data (classic use case)
Age
Income
Spending habits
No label called “customer type”.
Clustering reveals:
Budget buyers
Premium customers
Occasional shoppers
Then business teams act on those clusters.


Clustering is used when labels are unknown or undefined, to discover natural groupings in data; classification is used when labels are already known and the goal is to predict them.




Training a clustering model
There are multiple algorithms you can use for clustering. One of the most commonly used algorithms is K-Means clustering, which consists of the following steps:


The feature (x) values are vectorized to define n-dimensional coordinates (where n is the number of features). In the flower example, we have two features: number of leaves (x1) and number of petals (x2). So, the feature vector has two coordinates that we can use to conceptually plot the data points in two-dimensional space ([x1,x2])
You decide how many clusters you want to use to group the flowers - call this value k. For example, to create three clusters, you would use a k value of 3. Then k points are plotted at random coordinates. These points become the center points for each cluster, so they're called centroids.
Each data point (in this case a flower) is assigned to its nearest centroid.
Each centroid is moved to the center of the data points assigned to it based on the mean distance between the points.
After the centroid is moved, the data points may now be closer to a different centroid, so the data points are reassigned to clusters based on the new closest centroid.
The centroid movement and cluster reallocation steps are repeated until the clusters become stable or a predetermined maximum number of iterations is reached.


Evaluating a clustering model
Since there's no known label with which to compare the predicted cluster assignments, evaluation of a clustering model is based on how well the resulting clusters are separated from one another.


There are multiple metrics that you can use to evaluate cluster separation, including:


Average distance to cluster center: How close, on average, each point in the cluster is to the centroid of the cluster.
Average distance to another center: How close, on average, each point in the cluster is to the centroid of all other clusters.
Maximum distance to cluster center: The furthest distance between a point in the cluster and its centroid.
Silhouette: A value between -1 and 1 that summarizes the ratio of distance between points in the same cluster and points in different clusters (The closer to 1, the better the cluster separation).


Deep Learning
Deep learning is an advanced form of machine learning that tries to emulate the way the human brain learns. The key to deep learning is the creation of an artificial neural network that simulates electrochemical activity in biological neurons by using mathematical functions.


Each neuron is a function that operates on an input value (x) and a weight (w). The function is wrapped in an activation function that determines whether to pass the output on.


Artificial neural networks are made up of multiple layers of neurons - essentially defining a deeply nested function. This architecture is the reason the technique is referred to as deep learning and the models produced by it are often referred to as deep neural networks (DNNs). You can use deep neural networks for many kinds of machine learning problem, including regression and classification, as well as more specialized models for natural language processing and computer vision.


Just like other machine learning techniques discussed in this module, deep learning involves fitting training data to a function that can predict a label (y) based on the value of one or more features (x). The function (f(x)) is the outer layer of a nested function in which each layer of the neural network encapsulates functions that operate on x and the weight (w) values associated with them. The algorithm used to train the model involves iteratively feeding the feature values (x) in the training data forward through the layers to calculate output values for ŷ, validating the model to evaluate how far off the calculated ŷ values are from the known y values (which quantifies the level of error, or loss, in the model), and then modifying the weights (w) to reduce the loss. The trained model includes the final weight values that result in the most accurate predictions.


Lets take the penguin example where we have 4 features and we keep them in a vector 
The feature vector for a penguin observation is fed into the input layer of the neural network, which consists of a neuron for each x value. In this example, the following x vector is used as the input: [37.3, 16.8, 19.2, 30.0]
The functions for the first layer of neurons each calculate a weighted sum by combining the x value and w weight, and pass it to an activation function that determines if it meets the threshold to be passed on to the next layer.
Each neuron in a layer is connected to all of the neurons in the next layer (an architecture sometimes called a fully connected network) so the results of each layer are fed forward through the network until they reach the output layer.
The output layer produces a vector of values; in this case, using a softmax or similar function to calculate the probability distribution for the three possible classes of penguin. In this example, the output vector is: [0.2, 0.7, 0.1]
The elements of the vector represent the probabilities for classes 0, 1, and 2. The second value is the highest, so the model predicts that the species of the penguin is 1 (Gentoo).
















AI in Microsoft Foundry
Artificial Intelligence (AI) refers to systems designed to perform tasks that typically require human intelligence—such as reasoning, problem-solving, perception, and language understanding. Responsible AI: Emphasizes fairness, transparency, and ethical use of AI technologies.


AI is the broader goal—creating systems that mimic human intelligence. Machine learning (ML) is the primary method we use to reach AI and is made possible by data-driven algorithms. In general, ML enables machines to learn patterns from data and improve performance without explicit programming.


Types of ML:


Supervised and Unsupervised Learning: such as regression (supervised) for predicting prices, classification (supervised) for spam detection, and clustering (unsupervised) for customer segmentation.
Deep Learning: A specialized branch of ML using neural networks with multiple layers for tasks like image recognition and speech synthesis. Deep learning provides the foundation through neural networks that learn complex patterns from massive datasets.
Generative AI: uses deep learning capabilities to create new content—text, images, audio, code—rather than just classify or predict outcomes.


An AI application is a software solution that uses AI techniques—such as computer vision, speech, and information extraction—to perform tasks that typically require human-like intelligence. These applications can understand, reason, learn, and respond to inputs in a way that feels more adaptive and intelligent than traditional software.


AI applications are:


Model-powered: They use trained models to process inputs and generate outputs, such as text, images, or decisions.
Dynamic: Unlike static programs, AI apps can improve over time through retraining or fine-tuning.
Some of the typical ways people interact with AI applications include:


Conversational Interfaces: Users interact via chatbots or voice assistants (such as: asking questions, getting recommendations).
Embedded Features: AI is integrated into apps for tasks like autocomplete, image recognition, or fraud detection.
Decision Support: AI applications provide insights or predictions to help users make informed choices (such as: personalized shopping, medical diagnostics).
Automation: They handle repetitive tasks, such as document processing or customer service, reducing manual effort.
Some examples of AI applications for different industries include:


Healthcare: AI-powered diagnostic tools that analyze medical images (such as X-rays or MRIs) to help doctors detect diseases more accurately and quickly.
Finance: Fraud detection systems that use AI to monitor transactions in real time and identify suspicious activity, helping prevent financial crimes.
Retail: Personalized recommendation engines that analyze customer behavior and preferences to suggest products, improving the shopping experience.
Manufacturing: Predictive maintenance solutions that use AI to monitor equipment and forecast when machines are likely to fail, reducing downtime and maintenance costs.
Education: Intelligent tutoring systems that adapt to each student’s learning style and pace, providing customized feedback and support to enhance learning outcomes.


Components of an AI application


Data Layer
The data layer is the foundation of any AI application. It includes the collection, storage, and management of data used for training, inference, and decision-making. Common data sources include structured databases such as Azure SQL and PostgreSQL, unstructured data, such as documents and images, and real-time streams. Azure services like Cosmos DB and Azure Data Lake are often used to store and manage large-scale datasets efficiently.


Model Layer
The model layer involves the selection, training, and deployment of machine learning or AI models. Models can be pretrained (for example: Azure OpenAI in Foundry Models) or custom-built using platforms like Azure Machine Learning. This layer also includes tools for fine-tuning, evaluating, and versioning models to ensure they meet performance and accuracy requirements. Microsoft Foundry, a unified Azure platform-as-a-service for enterprise AI operations, provides a comprehensive model catalog for application developers.


Compute Layer
AI applications require compute resources to train and run models.


Integration & Orchestration Layer
The integration and orchestration layer connects models and data with business logic and user interfaces.


Microsoft Foundry for AI
Microsoft Foundry is a unified, enterprise-grade platform for building, deploying, and managing AI applications and agents. It consolidates models, agent orchestration, monitoring, and governance tools in one platform, offering production-grade infrastructure and security. With Foundry, developers can seamlessly design, customize, and scale generative AI applications using a rich portal experience or integrated SDKs, without worrying about underlying infrastructure complexities.


Understanding Azure
Microsoft Azure is a cloud computing platform that provides a wide range of services to help individuals and organizations build, deploy, and manage applications through Microsoft-managed data centers. It offers flexibility, scalability, and global reach, making it a popular choice for businesses of all sizes. Azure supports various programming languages, frameworks, and operating systems, allowing developers to work with the tools they prefer.


Cloud capabilities
Azure delivers core cloud capabilities across four main areas: compute, storage, networking, and application services. Compute services include virtual machines, containers, and serverless functions that run workloads efficiently. Storage services offer scalable and secure options for saving data, such as Blob Storage and Azure Files. Networking services connect resources securely and reliably, using tools like Azure Virtual Network and Load Balancer. Application services help developers build and host web apps, APIs, and mobile backends with ease.


Understand how Azure organizes your resources
Azure organizes access and management through a hierarchy of entities. A tenant represents a dedicated instance of Azure Active Directory, which handles identity and access management. Within a tenant, subscriptions define billing boundaries and provide access to Azure services. Each subscription can contain multiple resource groups, which are logical containers for managing related resources together. Resources are the individual services or components—like virtual machines, databases, or storage accounts—that you deploy and manage within Azure.


This organizational structure helps ensure clarity, security, and scalability in cloud environments. Tenants and subscriptions allow for clear separation of concerns across departments or projects. Resource groups simplify management by grouping related assets, making it easier to apply policies, monitor usage, and automate deployments. Understanding this hierarchy is essential for efficient cloud governance and cost control in Azure.


Foundry runs on Azure
Foundry runs on Azure and uses Azure resource types. Foundry is an AI development layer within Azure, designed to accelerate building and managing generative AI apps and agents with enterprise-grade governance. Foundry projects and hubs are resources that integrate with Azure networking, storage, and security.


Foundry Tools and models are cloud-based and accessed through a Foundry resource. This means that they are managed in the same way as other Azure services, such as platform as a service (PaaS), infrastructure as a service (IaaS), or a managed database service. From creating or deleting resources, to availability and billing, the Azure platform and resource manager provides a consistent framework for all your Azure services.


Thus, starting with an Azure subscription, you can create a Foundry project, and develop an AI application. Next, let's try starting a project in Foundry.




Introduction to text analysis concepts
Within artificial intelligence (AI), text analysis is a subset of natural language processing (NLP) that enables machines to extract meaning, structure, and insights from unstructured text. Organizations use text analysis to transform customer feedback, support tickets, contracts, and social media posts into actionable intelligence.


Techniques to process and analyze text evolved over many years, from simple statistical calculations based on term-frequency to vector-based language models that encapsulate semantic meaning. Some common use cases for text analysis include:


Key term extraction: Identifying important words and phrases in text, to help determine the topics and themes it discusses.
Entity detection: Identifying named entities mentioned in text; for example, places, people, dates, and organizations.
Text classification: Categorizing text documents based on their contents. For example, filtering email as spam or not spam.
Sentiment analysis: A particular form of text classification that predicts the sentiment of text - for example, categorizing social media posts as positive, neutral, or negative.
Text summarization: Reducing the volume of text while retaining its salient points. For example, generating a short one-paragraph summary from a multi-page document.
Text analysis is challenging because language is complex, and computers find it hard to understand. Ultimately, all text analysis techniques are based on the requirement to extract meaning from natural language text.






Tokenization
The first step in analyzing a body of text (referred to as a corpus) is to break it down into tokens. For the sake of simplicity, you can think of each distinct word in the text as a token. In reality, tokens can be generated for partial words or combinations of words and punctuation.


We've used a simple example in which tokens are identified for each distinct word in the text. However, consider the following pre-processing techniques that might apply to tokenization depending on the specific text analysis problem you're trying to solve:




Technique
	Description
	Text normalization
	Before generating tokens, you might choose to normalize the text by removing punctuation and changing all words to lower case. For analysis that relies purely on word frequency, this approach improves overall performance. However, some semantic meaning could be lost - for example, consider the sentence "Mr Banks has worked in many banks.". You may want your analysis to differentiate between the person "Mr Banks" and the "banks" in which he's worked. You might also want to consider "banks." as a separate token to "banks" because the inclusion of a period provides the information that the word comes at the end of a sentence
	Stop word removal
	Stop words are words that should be excluded from the analysis. For example, "the", "a", or "it" make text easier for people to read but add little semantic meaning. By excluding these words, a text analysis solution might be better able to identify the important words.
	N-gram extraction
	Finding multi-term phrases such as "artificial intelligence" or "natural language processing". A single word phrase is a unigram, a two-word phrase is a bigram, a three-word phrase is a trigram, and so on. In many cases, by considering frequently appearing sequences of words as groups, a text analysis algorithm can make better sense of the text.
	Stemming
	A technique used to consolidate words by stripping endings like "s", "ing", "ed", and so on, before counting them; so that words with the same etymological root, like "powering", "powered", and "powerful", are interpreted as being the same token ("power").
	Lemmatization
	Another approach to reducing words to their base or dictionary form (called a lemma). Unlike stemming, which simply chops off word endings, lemmatization uses linguistic rules and vocabulary to ensure the resulting form is a valid word (for example, "running": → "run", "global" → "globe").
	Parts of speech (POS) tagging
	Labeling each token with its grammatical category, such as noun, verb, adjective, or adverb. This technique uses linguistic rules and often statistical models to determine the correct tag based on both the token itself and its context within the sentence.
	

Statistical text analysis
Having broken down a text corpus into its constituent tokens, and prepared them for analysis; there are some common statistical analysis techniques you can use to infer meaning from the text.


Frequency Analysis
Perhaps the most obvious way to ascertain the topics discussed in a document is to simply count the number of times each normalized token appears. The assumption is that terms that are used more frequently in the document can help identify the subjects or themes discussed. Put simply, if you can determine the most commonly used words in a given document, you can often get a good idea of what the document is about.
Tokenization → splits text into tokens
Normalization → standardizes those tokens
Normalized token = tokenized unit + normalization applied
A normalized token is a token that has been standardized through steps like lowercasing, stemming or lemmatization, and punctuation removal so that different surface forms of the same word are treated as one.
Run, Running, Ran the count becomes 1,1,1 for each word but if we normalize it to “run” we can write count as 3 directly
After normalization run → run, running → run and ran → run


Term Frequency - Inverse Document Frequency (TF-IDF)
Simple frequency analysis in which you count the number of occurrences of each token can be an effective way to analyze a single document, but when you need to differentiate across multiple documents within the same corpus, you need a way to determine which tokens are most relevant in each individual document.


Example →
Step 1: A tiny document collection (corpus)


Imagine we have 3 documents:
Document 1 (D1)
“AI agent builds agent workflows using LLM”


Document 2 (D2)
“AI model training requires data and compute”


Document 3 (D3)
“Agent orchestration and agent monitoring”


We want to know which words best describe each document.
Lets go with Document 1


Step 2: Term Frequency (TF)
TF = How many times a word appears in THIS document
Let’s calculate TF for a few words.


TF in Document 1 (D1)
Word
	Count (TF)
	agent
	2
	ai
	1
	workflows
	1
	llm
	1
	

tf(agent, D1) = 2
tf(ai, D1) = 1
And so on


Step 3: Document Frequency (df)
df = In how many documents does this word appear?
We check across all 3 documents.




Word
	Appears in Docs
	df
	agent
	D1, D3
	2
	ai
	D1, D2
	2
	workflows
	D1 only
	1
	llm
	D1 only
	1
	and
	D2, D3
	2
	

Total documents: N = 3


Step 4: Inverse Document Frequency (IDF)
Formula → idf(t) = log(N / df(t))


IDF values
Word
	df
	IDF = log(3/df)
	agent
	2
	log(1.5) ≈ 0.18
	ai
	2
	log(1.5) ≈ 0.18
	workflows
	1
	log(3) ≈ 0.48
	llm
	1
	log(3) ≈ 0.48
	

Key intuition:
Words appearing in many documents → low IDF
Words appearing in few documents → high IDF


Step 5: TF-IDF = TF × IDF












Word
	TF
	IDF
	TF-IDF
	agent
	2
	0.18
	0.36
	ai
	1
	0.18
	0.18
	workflows
	1
	0.48
	0.48
	llm
	1
	0.48
	0.48
	

Interpretation (THIS is the real understanding)
Even though "agent" appears the most (TF=2):
It also appears in another document
So its IDF is low
Result: TF-IDF is not very high


Meanwhile:
"workflows" and "llm"
Appear only in this document
So they get higher TF-IDF
They describe what makes this document special


Final takeaway for D1:
This document is more about LLM workflows than just “agent”.


Here is flow understanding


We focus on one document (say D1) and compute Term Frequency (TF), which counts how many times each word appears in that document.


Then we compute Document Frequency (DF), which tells us in how many documents across the corpus the word appears — some documents may not contain the word at all.


Next, we compute Inverse Document Frequency (IDF), which gives higher values to words that appear in fewer documents and lower values to words that appear in many documents.


Finally, we multiply TF and IDF. Words with high TF-IDF scores are frequent in the document but rare across the corpus, making them good descriptors of what the document is about.


We use log in IDF because:
Without log, rare words become too powerful
We want diminishing returns, not linear growth
It stabilizes scores across large corpora
And no — log is not the only way, but it is the standard way because it works well.


The logarithm in IDF compresses large frequency differences. It increases rapidly for rare terms but flattens as document frequency grows, ensuring that very rare words are important without dominating the score, while common words approach zero importance.
It is like the log graph where initially it gives high results but then flatten out and this is what we get if say there is a 1 word in say 1000000 word doc then generally if we do not use log and say use something like this
df = 1
IDF = N / df = 1,000,000
We get a IDF os 1000000 which is huge and say we get 10 somewhat rare word then the IDF becomes 100000 the diff between rare words is huge we need to minimise this and so we use log since , if the value log of 1000000 is nearly 6 and log of 100000 is nearly 5 so the diff here is just 1
Hence we use log


“Bag-of-words" machine learning techniques
Bag-of-Words is just a way to convert text into numbers. It builds a vocabulary or we can say list of words, represents each document as a vector of counts, and ignores order and grammar.
BoW does not classify it just creates a feature, an array of words


Lets say we have 4 emails


Email
	Text
	Label
	

E1
	“win money now”
	Spam
	

E2
	“limited offer win prize”
	Spam
	

E3
	“meeting schedule tomorrow”
	Not Spam
	

E4
	“project meeting update”
	Not Spam
	

BoW - [ win, money, now, limited, offer, prize,
  meeting, schedule, tomorrow, project, update ]
Example of email 1 -> E1 = “win money now”








Word
	Count
	win
	1
	money
	1
	now
	1
	others
	0
	Vector - [1,1,1,0,0,0,0,0,0,0,0]


E3 = “meeting schedule tomorrow”
Vector - [0,0,0,0,0,0,1,1,1,0,0]


Bayes’ Theorem (plain English)
P(Class | Words) ∝ P(Class) × P(Words | Class)


Probability that an email is Spam given the words it contains
Or we can also say it as 
Given these words how likely is each class? Why each class because once we know one class probability we can determine the other class probability by subtracting it by 1


Naive Bayes assumes:
Each word contributes independently


P(win money now | Spam)
≈ P(win | Spam) × P(money | Spam) × P(now | Spam)


Training data table (conceptually)
Email Text
	BoW Vector
	Label
	“win money now”
	[1,0,0,1,0,…]
	Spam
	“meeting tomorrow”
	[0,1,1,0,0,…]
	Not Spam
	

This is where we get to know the label, we just have one big array of words called BoW and then while training the model we use this label to determine which words or vector combination is of type spam and which is not


In Bag-of-Words, we build a single global vocabulary and represent each document as a word-count vector using that vocabulary. The spam or non-spam information is stored as a separate label, and the classifier learns class-specific word probabilities during training.


Bayes Theorem Formula →


  



TextRank
TextRank is an unsupervised graph-based algorithm that models text as a network of connected nodes. For example, each sentence in a document could be considered a node, and the connections (edges) between them are scored based on the similarity of the words they contain. TextRank is commonly used to summarize text based on identifying a subset of sentences within a document that best represent its overall subject.


The TextRank algorithm applies the same principle as Google's PageRank algorithm (which ranks web pages based on links between them) to text. The key idea is that a sentence is important if it's similar to many other important sentences. The algorithm works through the following steps:


Build a graph: Each sentence becomes a node, and edges that connect them are weighted by similarity (often measured using word overlap or cosine similarity between sentence vectors).


Calculate ranks iteratively: Each node's score is calculated based on the scores of the nodes connected to it. The formula is: TextRank(Sᵢ) = (1-d) + d * Σ(wⱼᵢ / Σwⱼₖ) * TextRank(Sⱼ) (where d is a damping factor, typically 0.85, wⱼᵢ is the weight of the edge from sentence j to sentence i, and the sum iterates over all sentences connected to i).


Extract top-ranked sentences: After convergence, the sentences with the highest scores are selected as the summary.


TextRank can also be applied at the word level for keyword extraction, where words (rather than sentences) become nodes, and edges represent co-occurrence within a fixed window. The highest-ranked words are extracted as key terms representing the document's main topics.


For example, consider the following document about cloud computing:


Cloud computing provides on-demand access to computing resources. Computing resources include servers, storage, and networking. Azure is Microsoft's cloud computing platform. Organizations use cloud platforms to reduce infrastructure costs. Cloud computing enables scalability and flexibility.


To generate a summary of this document, the TextRank process begins by splitting this document into sentences:


Cloud computing provides on-demand access to computing resources.
Computing resources include servers, storage, and networking.
Azure is Microsoft's cloud computing platform.
Organizations use cloud platforms to reduce infrastructure costs.
Cloud computing enables scalability and flexibility.


Next, edges are created between sentences with weights based on similarity (word overlap). For this example, the edge weights might be:


Sentence 1 <-> Sentence 2: 0.5 (shares "computing resources")
Sentence 1 <-> Sentence 3: 0.6 (shares "cloud computing")
Sentence 1 <-> Sentence 4: 0.2 (shares "cloud")
Sentence 1 <-> Sentence 5: 0.7 (shares "cloud computing")
Sentence 2 <-> Sentence 3: 0.2 (limited overlap)
Sentence 2 <-> Sentence 4: 0.1 (limited overlap)
Sentence 2 <-> Sentence 5: 0.1 (shares "computing")
Sentence 3 <-> Sentence 4: 0.5 (shares "cloud platforms")
Sentence 3 <-> Sentence 5: 0.4 (shares "cloud computing")
Sentence 4 <-> Sentence 5: 0.3 (limited overlap)


After calculating TextRank scores iteratively using these weights, sentences 1, 3, and 5 might receive the highest scores because they connect well to other sentences through shared terminology and concepts. These sentences would be selected to form a concise summary: "Cloud computing provides on-demand access to computing resources. Azure is Microsoft's cloud computing platform. Cloud computing enables scalability and flexibility."


Generating a document summary by selecting the most relevant sentences is a form of extractive summarization. In this approach, no new text is generated - the summary consists of a subset of the original text. More recent developments in semantic modeling also enable abstractive summarization, in which new language that summarizes the key themes of the source document is generated.


Why s1-s3 has less score than s1-s5 despite being same words


Those numbers (0.5, 0.6, 0.7) are illustrative, not deterministic, and in real TextRank they come from a sentence similarity formula, not just “same words = same score”.


In TextRank, sentence similarity is computed pairwise using word overlap normalized by sentence length (or TF-IDF/embeddings in modern variants), so two pairs with the same shared words can still receive different weights due to normalization and semantic closeness.


How sentence similarity is actually computed in TextRank
  

So similarity depends on three things:


1. Number of shared words
More overlap → higher score


2. Sentence length normalization
Shorter sentences sharing the same words → higher similarity


3. Exact token match after normalization
Lowercase, stopword removal, stemming, etc.